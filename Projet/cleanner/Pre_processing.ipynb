{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pré-processing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "d_Azb0uBqFS7",
        "Qi5SFtl9qbXA",
        "-t8CO_YxqyyB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/nkcr/WEM-labs/blob/master/Projet/cleanner/Pre_processing.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "uqq8vZ7F94LQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ce notebook contient les scripts pour le projet Per-Tempus"
      ]
    },
    {
      "metadata": {
        "id": "2JExyRFX9_At",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pré-traitement"
      ]
    },
    {
      "metadata": {
        "id": "dIk726UxsHZI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set the mongoDB configuration URL. DO NOT LEAVE IT HERE FOR GIT VERSIONING"
      ]
    },
    {
      "metadata": {
        "id": "xcYJoq_Eo1xN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MD=\"???\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I3Ill8kHavRf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies and Mongo Client"
      ]
    },
    {
      "metadata": {
        "id": "mRtCmg9Vas_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "4orPU1f3920n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "63fe7768-cc52-4099-c344-6b1c6831576c"
      },
      "cell_type": "code",
      "source": [
        "!pip install pymongo\n",
        "\n",
        "from pymongo import MongoClient\n",
        "import numpy as np\n",
        "\n",
        "client = MongoClient(MD)\n",
        "db = client.news"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymongo\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/f9/78dd244df932309299288a452d1c3524f6f7746f1813b8a8417952b1d9ce/pymongo-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (378kB)\n",
            "\u001b[K    100% |████████████████████████████████| 389kB 6.4MB/s \n",
            "\u001b[?25hInstalling collected packages: pymongo\n",
            "Successfully installed pymongo-3.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sG9eziqdaxAA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a10ed562-8e26-4fcd-c17c-9a033df87b70"
      },
      "cell_type": "code",
      "source": [
        "print(db.swissinfo.find().count())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "siJANSXc-Dom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc5552ad-a073-4ed9-fcb3-949db77dcc67"
      },
      "cell_type": "code",
      "source": [
        "source = [\n",
        "    \"swissinfo\",\n",
        "    \"rts\",\n",
        "    \"leTemps\",\n",
        "    \"20Minutes\"\n",
        "]\n",
        "\n",
        "totals = list( map( lambda collection: db[collection].find().count(), source ) )\n",
        "total = np.sum(totals)\n",
        "\n",
        "print(total)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "133228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xur3epnD43-Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate RAW corpus"
      ]
    },
    {
      "metadata": {
        "id": "MkIYgCiYbUTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3b20fa23-8fb5-460e-aa01-80fb22798c60"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from multiprocessing import Pool\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# The leTemps dataset has 159534 different tokens\n",
        "\n",
        "corpus = set()\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"Original taken\n",
        "    from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9()èéàÉÀêÊçÇ]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "  \n",
        "def tokenize(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "for article in db.allArticles.find({},{\"clearedHTML\":1}):\n",
        "  [corpus.add(token) for token in tokenize(clean_str(article[\"clearedHTML\"]))]\n",
        "  \n",
        "print(len(corpus))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "414738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mwpa3fcgg7eK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2304219d-e729-41d7-aa7e-3f1a54a32f97"
      },
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414738"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "metadata": {
        "id": "pJUg-_zS49Ai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Save RAW corpus"
      ]
    },
    {
      "metadata": {
        "id": "b1HlFFvdYXgQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "db_corpus = client.corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OBpfplhrrYe2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b3e8e12-5bc7-4aec-ff09-559706ac85b0"
      },
      "cell_type": "code",
      "source": [
        "list_corpus = list(corpus)\n",
        "db_corpus.allArticles.insert_one(\n",
        "    {\n",
        "        \"id\": \"raw2\",\n",
        "        \"corpus\": list_corpus,\n",
        "        \"description\": \"uniq words tokenized by nltk on all articles. Words are\\\n",
        "          only processed with `clean_str` UPDATE WITH ACCENTS\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pymongo.results.InsertOneResult at 0x7f94865e2588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "metadata": {
        "id": "UwNUx2Mb5C9q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get RAW corpus, remove small tokens and add it to Mongo\n",
        "\n",
        "We estimate that the collection contains only one document"
      ]
    },
    {
      "metadata": {
        "id": "hFU9-08yyNJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list_raw_corpus = db_corpus.allArticles.find_one({\"id\": \"raw2\"})[\"corpus\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ywMgqEXP6pMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f17f5463-1e3a-4e7d-fde3-66e353280d4d"
      },
      "cell_type": "code",
      "source": [
        "print(len(list_corpus))\n",
        "\n",
        "larger = list(filter(lambda w: len(w) > 2, list_corpus))\n",
        "\n",
        "print(len(larger))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "414738\n",
            "413630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-GrXv8yM7COr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d6c780b7-2f1f-4d47-ac9f-87e7193bd6ee"
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# An example to add a field\n",
        "#\n",
        "#db_corpus.allArticles.find_one_and_update( {},{'$set':{\"raw_more_than_two_chars\": larger}} )\n",
        "\n",
        "db_corpus.allArticles.insert_one(\n",
        "    {\n",
        "        \"id\": \"greater_than_two2\",\n",
        "        \"corpus\": larger,\n",
        "        \"description\": \"Raw corpus with only tokens greater than 2 chars WITH UPDATED ACCENTS\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pymongo.results.InsertOneResult at 0x7f948d321608>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "metadata": {
        "id": "zKEZve8RlkXW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Remove numeric tokens and add it"
      ]
    },
    {
      "metadata": {
        "id": "JORzyS34Ytw9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0abb5377-abc4-43eb-8399-aa2b590d94f2"
      },
      "cell_type": "code",
      "source": [
        "print(len(larger))\n",
        "\n",
        "condition = lambda w: not w.replace('\"', '').replace(\"'\", '')\\\n",
        "          .replace(\".\", '').replace(\",\", '').isnumeric()\n",
        "\n",
        "alpha_only = list(filter(condition, larger))\n",
        "\n",
        "print(len(alpha_only))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "413630\n",
            "407229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yN1Rr14il25W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7c23a16-353a-4749-81bf-26b6f342108c"
      },
      "cell_type": "code",
      "source": [
        "db_corpus.allArticles.insert_one(\n",
        "    {\n",
        "        \"id\": \"only_alpha2\",\n",
        "        \"corpus\": alpha_only,\n",
        "        \"description\": \"`greater_than_two` without numeric tokens WITH UPDATED ACCENT\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pymongo.results.InsertOneResult at 0x7f94865efb08>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "metadata": {
        "id": "ObWp8cQYt9FL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Special chars and lower"
      ]
    },
    {
      "metadata": {
        "id": "OTkYj1_FvUav",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RGZV_ZGSuYeq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "db_corpus = client.corpus\n",
        "alpha_only = db_corpus.allArticles.find_one({\"id\": \"only_alpha2\"})[\"corpus\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m9AeV8KNt_q1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4783ee79-6c64-4c44-9a68-4cf712904ef9"
      },
      "cell_type": "code",
      "source": [
        "print(len(alpha_only))\n",
        "\n",
        "def clean_str(string):\n",
        "  \"\"\"Original taken\n",
        "  from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "  \"\"\"\n",
        "  string = re.sub(r\"[^A-Za-z0-9()èéàÉÀêÊçÇ]\", \" \", string)\n",
        "  string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "  string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "  string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "  string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "  string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "  string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "  string = re.sub(r\",\", \" , \", string)\n",
        "  string = re.sub(r\"!\", \" ! \", string)\n",
        "  string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "  string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "  string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "  string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "  return word_tokenize(string.strip().lower())\n",
        "\n",
        "cleaned = [el for arr in list(map(lambda w: clean_str(w), alpha_only)) for el in arr ]\n",
        "cleaned = list(filter(lambda w: len(w) > 2 and not w.replace('\"', '').replace(\"'\", '').replace(\".\", '').replace(\",\", '').isnumeric(), cleaned))\n",
        "print(len(cleaned))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "407229\n",
            "407229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7Z7Z5Dw8u2y_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "a8ec50b7-aaff-4c28-ec77-d2bd22a3fb84"
      },
      "cell_type": "code",
      "source": [
        "cleaned[:10]"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fanatiques',\n",
              " 'redgravec',\n",
              " 'rience',\n",
              " 'buseno',\n",
              " 'urgenthigh',\n",
              " 'croustillants',\n",
              " 'breukelen',\n",
              " 'froidhillary',\n",
              " 'noircies',\n",
              " 'tokihiro']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "metadata": {
        "id": "NpZNfXVD1Km5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "76ce7d8a-9541-44f8-f8c0-885207fc85ed"
      },
      "cell_type": "code",
      "source": [
        "db_corpus.allArticles.insert_one(\n",
        "    {\n",
        "        \"id\": \"special_chars2\",\n",
        "        \"corpus\": cleaned,\n",
        "        \"description\": \"`only_alpha` without without special chars. WITH UPDATED ACCENT\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pymongo.results.InsertOneResult at 0x7f9491864188>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "metadata": {
        "id": "mKKE6tm8oof4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### French stemming"
      ]
    },
    {
      "metadata": {
        "id": "0NQTbFJsoqYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f8f2095e-52e7-4b5b-d320-ac4b2aa0db87"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"french\")\n",
        "\n",
        "print(len(alpha_only))\n",
        "\n",
        "french_stemmed = list(map(lambda w: stemmer.stem(w), alpha_only))\n",
        "french_stemmed = list(set(french_stemmed))\n",
        "\n",
        "print(len(french_stemmed))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "390040\n",
            "305889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6TYR38yUo3UX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c395111e-3328-4b30-d7d3-06fa9d60d9e1"
      },
      "cell_type": "code",
      "source": [
        "db_corpus.allArticles.insert_one(\n",
        "    {\n",
        "        \"id\": \"alpha_french_stemmed\",\n",
        "        \"corpus\": french_stemmed,\n",
        "        \"description\": \"`greater_than_two` without numeric tokens and french\\\n",
        "                        stemmed\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pymongo.results.InsertOneResult at 0x7f4306a57a70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "d_Azb0uBqFS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### English stemming"
      ]
    },
    {
      "metadata": {
        "id": "g39Vde5do5pO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "83ed257f-e6a5-43fe-cf49-ac3ca26457be"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "print(len(alpha_only))\n",
        "\n",
        "english_stemmed = list(map(lambda w: stemmer.stem(w), alpha_only))\n",
        "english_stemmed = list(set(english_stemmed))\n",
        "\n",
        "print(len(english_stemmed))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "390040\n",
            "335813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jO7-y6RdqPmf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "624df1e5-6723-4ec1-d080-5aa6f0c5d8aa"
      },
      "cell_type": "code",
      "source": [
        "db_corpus.allArticles.insert_one(\n",
        "    {\n",
        "        \"id\": \"alpha_english_stemmed\",\n",
        "        \"corpus\": english_stemmed,\n",
        "        \"description\": \"`greater_than_two` without numeric tokens and english\\\n",
        "                        stemmed\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pymongo.results.InsertOneResult at 0x7f43156a4368>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "Qi5SFtl9qbXA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### French and English stemmed"
      ]
    },
    {
      "metadata": {
        "id": "epTib4BOqaeS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "541cbb6f-0474-4baf-839d-ad730098f543"
      },
      "cell_type": "code",
      "source": [
        "print(len(french_stemmed))\n",
        "\n",
        "french_english_stemmed = list(map(lambda w: stemmer.stem(w), french_stemmed))\n",
        "french_english_stemmed = list(set(french_english_stemmed))\n",
        "\n",
        "print(len(french_english_stemmed))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "305889\n",
            "292923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C1a83jdhqZR4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92184cfd-8782-4e7d-cbdc-50a84591ae57"
      },
      "cell_type": "code",
      "source": [
        "db_corpus.allArticles.insert_one(\n",
        "    {\n",
        "        \"id\": \"alpha_english_french_stemmed\",\n",
        "        \"corpus\": french_english_stemmed,\n",
        "        \"description\": \"`greater_than_two` without numeric tokens and french\\\n",
        "                        and english stemmed\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pymongo.results.InsertOneResult at 0x7f4308169830>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "7IIDg6k0q7cA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_xlUNSlVBDJL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Add clean words field in all articles"
      ]
    },
    {
      "metadata": {
        "id": "jn2dl0xdBNHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "275946cb-788f-4062-f277-901cf5f60185"
      },
      "cell_type": "code",
      "source": [
        "!pip install pymongo\n",
        "\n",
        "from pymongo import MongoClient\n",
        "import numpy as np\n",
        "\n",
        "client = MongoClient(MD)\n",
        "db = client.news"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymongo\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/5a/77060da2196471c8c47eeed6526029bd35cb2f10b1e4fc0e5e5234ca1aa0/pymongo-3.6.1-cp27-cp27mu-manylinux1_x86_64.whl (381kB)\n",
            "\u001b[K    100% |████████████████████████████████| 389kB 5.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pymongo\n",
            "Successfully installed pymongo-3.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ac3XRksxBX9D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "db_corpus = client.corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G_KZ_GDHBdSF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = db_corpus.allArticles.find_one({\"id\": \"alpha_english_french_stemmed\"})[\"corpus\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iSQAj1hRCT94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "391948c3-00d0-4cb4-85a1-291295bc61e0"
      },
      "cell_type": "code",
      "source": [
        "print(corpus[:10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[u'foufoun', u'ledirectoir', u'troproug', u'gavap', u'woodi', u'gah', u'daiich', u'juvent', u'woodr', u'dwaar']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NG0qXTuRnec3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is an attempt to update the corpus. Will be finaly done elsewhere for performane concerns."
      ]
    },
    {
      "metadata": {
        "id": "bDZkopRQCXg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "711b0d15-20bd-48e4-b534-53c62e00cf9b"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmerFR = SnowballStemmer(\"french\")\n",
        "stemmerEN = SnowballStemmer(\"english\")\n",
        "import re\n",
        "import threading\n",
        "\n",
        "alphaOnly = lambda w: not w.replace('\"', '').replace(\"'\", '')\\\n",
        "          .replace(\".\", '').replace(\",\", '').isnumeric()\n",
        "\n",
        "gt2 = lambda w: len(w) > 2\n",
        "\n",
        "ppFilters = lambda w: alphaOnly(w) and gt2(w)\n",
        "\n",
        "def clean_str(string):\n",
        "  \"\"\"Original taken\n",
        "  from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "  \"\"\"\n",
        "  string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "  string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "  string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "  string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "  string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "  string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "  string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "  string = re.sub(r\",\", \" , \", string)\n",
        "  string = re.sub(r\"!\", \" ! \", string)\n",
        "  string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "  string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "  string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "  string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "  return string.strip().lower()\n",
        "\n",
        "stems = lambda w: stemmerFR.stem(w) and stemmerEN.stem(w)\n",
        "\n",
        "def ppMaps(w):\n",
        "  #w = clean_str(w)\n",
        "  w = stems(w)\n",
        "  return w\n",
        "\n",
        "def preprocessing(tokens):\n",
        "  tokens = list(filter(ppFilters, map(ppMaps, tokens)))\n",
        "  return tokens\n",
        "\n",
        "def sec_process():\n",
        "  print(\"Start...\")\n",
        "  i = 0\n",
        "  #db_corpus.allArticles.find_one_and_update( {},{'$set':{\"raw_more_than_two_chars\": larger}} )\n",
        "  for article in db.allArticles.find():\n",
        "    i = i+1\n",
        "    db.allArticles.update_one({'_id':article[\"_id\"]}, {\"$set\": {\"cleanContent\": \" \".join(preprocessing(word_tokenize(clean_str(article['clearedHTML']))))}}, upsert=False)\n",
        "    if i % 1000 == 0:\n",
        "      print(\"> {}\", i)\n",
        "\n",
        "  print(\"...Done!\")\n",
        "\n",
        "\n",
        "class ProcessDocs:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.i = [0]\n",
        "    cursors = db.allArticles.parallel_scan(4)\n",
        "    threads = [threading.Thread(target=self.process_cursor, args=(cursor,self.i)) for cursor in cursors]\n",
        "\n",
        "    for thread in threads:\n",
        "      thread.start()\n",
        "\n",
        "    for thread in threads:\n",
        "      thread.join()\n",
        "\n",
        "  def process_document(self, document, i):\n",
        "    i[0] = i[0]+1\n",
        "    db.allArticles.update_one({'_id':document[\"_id\"]}, {\"$set\": {\"cleanContent\": \" \".join(preprocessing(word_tokenize(clean_str(document['clearedHTML']))))}}, upsert=False)\n",
        "    if self.i[0] % 1000 == 0:\n",
        "      print(\"> %d\", i)\n",
        "\n",
        "  def process_cursor(self, cursor, i):\n",
        "    for document in cursor:\n",
        "      # Some thread-safe processing function:\n",
        "      self.process_document(document, i)\n",
        "      \n",
        "ProcessDocs()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "('> %d', [1000])\n",
            "('> %d', [2000])\n",
            "('> %d', [3000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MDNlZ94oL6SL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ElyNmX7pC_e_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-t8CO_YxqyyB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenize and clean for vectorizer TF-IDF ©"
      ]
    },
    {
      "metadata": {
        "id": "djPiYj7Nq4E-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9a5649d7-827d-4307-90a9-f94ad8c36941"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmerFR = SnowballStemmer(\"french\")\n",
        "stemmerEN = SnowballStemmer(\"english\")\n",
        "import re\n",
        "from scipy import optimize\n",
        "\n",
        "alphaOnly = lambda w: not w.replace('\"', '').replace(\"'\", '')\\\n",
        "          .replace(\".\", '').replace(\",\", '').isnumeric()\n",
        "\n",
        "gt2 = lambda w: len(w) > 2\n",
        "\n",
        "ppFilters = lambda w: alphaOnly(w) and gt2(w)\n",
        "\n",
        "def clean_str(string):\n",
        "  \"\"\"Original taken\n",
        "  from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "  \"\"\"\n",
        "  string = re.sub(r\"[^A-Za-z0-9()èéàÉÀêÊçÇ]\", \" \", string)\n",
        "  string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "  string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "  string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "  string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "  string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "  string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "  string = re.sub(r\",\", \" , \", string)\n",
        "  string = re.sub(r\"!\", \" ! \", string)\n",
        "  string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "  string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "  string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "  string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "  return string.strip().lower()\n",
        "\n",
        "\n",
        "def ppMaps(w):\n",
        "  #w = clean_str(w)\n",
        "  w = stemmerFR.stem(w)\n",
        "  #w = stemmerEN.stem(w)\n",
        "  return w\n",
        "\n",
        "def preprocessing(tokens):\n",
        "  tokens = list(filter(ppFilters, map(ppMaps, tokens)))\n",
        "  return tokens\n",
        "\n",
        "def process_line(line):\n",
        "  return preprocessing(word_tokenize(clean_str(line)))\n",
        "\n",
        "# Call `pro£cess_line` to use it."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3X1DcBVArAa4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fba685f1-1c61-453a-bacc-a26907ad91ee"
      },
      "cell_type": "code",
      "source": [
        "process_line(\"Hello loïc. Are you there?\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'are', 'you', 'ther']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "XpSZrbgSrEB_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WJ4t-kXltDU8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create Stemmed <> Original pairs based on alpha_only"
      ]
    },
    {
      "metadata": {
        "id": "9RKmavZt3kv4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"french\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "92ZjusmetJtk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "db_corpus = client.corpus\n",
        "corpus = db_corpus.allArticles.find_one({\"id\": \"special_chars2\"})[\"corpus\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LeZnbOM9tt5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "583c5565-f869-4a1e-c3a9-41c10df48943"
      },
      "cell_type": "code",
      "source": [
        "print(corpus[:10])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fanatiques', 'redgravec', 'rience', 'buseno', 'urgenthigh', 'croustillants', 'breukelen', 'froidhillary', 'noircies', 'tokihiro']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vYng93d2txXK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pairs = {}\n",
        "for w in corpus:\n",
        "  pairs[w] = stemmer.stem(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VYaHuB5D3zFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "76bc69ff-5dce-4912-8440-84b6711d6d81"
      },
      "cell_type": "code",
      "source": [
        "db_corpus.allArticles.insert_one(\n",
        "    {\n",
        "        \"id\": \"pairs2\",\n",
        "        \"pairs\": pairs,\n",
        "        \"description\": \"each original word from special_chars` with their corresponding french stemming WITH UPDATED ACCENT\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pymongo.results.InsertOneResult at 0x7f948ac38448>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "metadata": {
        "id": "uvPOTlC935HP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pairs_got = db_corpus.allArticles.find_one({\"id\": \"pairs\"})[\"pairs\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yy-axYTd4bZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cc35df95-3cac-4bef-e0e8-796ed8bc1085"
      },
      "cell_type": "code",
      "source": [
        "print(list(pairs_got.keys())[:10])\n",
        "print(list(pairs_got.values())[:10])\n",
        "print(len(list(pairs_got.values())))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['poliscanova', 'applepartage', 'sowell', 'administreront', 'woods', 'sunde', 'bolognelundi', 'unescosouligne', 'woody', 'hennings']\n",
            "['poliscanov', 'applepartag', 'sowel', 'administr', 'wood', 'sund', 'bolognelund', 'unescosoulign', 'woody', 'henning']\n",
            "360447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t4drId8u7WpO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}